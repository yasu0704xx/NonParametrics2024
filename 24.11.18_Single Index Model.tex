\documentclass[xcolor=svgnames,dvipdfmx,cjk]{beamer} 
\AtBeginDvi{\special{pdf:tounicode 90ms-RKSJ-UCS2}} 
\usetheme{metropolis}
\usefonttheme{professionalfonts}
\setbeamertemplate{theorems}[numbered]
\newtheorem{thm}{Theorem}[section]
\newtheorem{proposition}[thm]{Proposition}
\theoremstyle{example}
\newtheorem{exam}[thm]{Example}
\newtheorem{remark}[thm]{Remark}
\newtheorem{question}[thm]{Question}
\newtheorem{prob}[thm]{Problem}
\usepackage{bbm}
\usepackage{ascmac}

\begin{document} 

%%%%%講演に関する情報%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[Li and Racine (2007, Chapter 8)]{Semiparametric Single Index Models} 
\author[Y. Matsumura]{Yasuyuki Matsumura}          
\institute[]{Graduate School of Economics, Kyoto University} 
\date{\today}


%%%%%タイトルページ%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}                  
\titlepage                     
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Before showwing the contents of the slides...

\begin{frame}{Introduction}
  \begin{itemize}
    \item  A semiparametric single index model is given by 
            \begin{align*}
              Y = g (X^{T} \beta_0) + u,
            \end{align*}
           where  
            \begin{align*}
              & Y \in \mathbb{R}: \text{a dependent variable}, \\
              & X \in \mathbb{R}^{q}: \text{a }  q \times 1 \text{ explanatory vector}, \\
              & \beta_0 \in \mathbb{R}^{q}: \text{a }  q \times 1 \text{ vector of unknown parameters}, \\
              & u \in \mathbb{R}: \text{an error term which satisfies } \mathbb{E}(u \mid X) =0, \\
              & g(\cdot): \text{an unknown distribution function}.
            \end{align*}
  \end{itemize}
\end{frame}

\begin{frame}{Introduction}
  \begin{itemize}
    \item Even though $x$ is a $q\times1$ vector, 
          $x^{T} \beta_0$ is a scalar of a single linear combination, 
          which is called a single index.
    \item By the form of the single index model, we obtain
          \begin{align*}
            \mathbb{E}(Y \mid X) = g(X^{T} \beta_0),
          \end{align*}
          which means that 
          the conditional expectation of $Y$ 
          only depends on the vector $X$
          through a single index $X^{T} \beta_0$.
    \item The model is SEMIPARAMETRIC 
          when $\beta \in \mathbb{R}^{q}$ is estimated with the parametric methods
          and $g(\cdot)$ with the nonparametric methods.
    \item Some of the PARAMETRIC single index models are really familiar with us.
  \end{itemize}
\end{frame}

\begin{frame}{Examples of Parametric Single Index Model}
  \begin{itemize}
    \item If $g(\cdot)$ is the identity function, 
          then the model turns out to be a linear regression model:
          \begin{align*}
            Y = g (X^{T} \beta_0) + u = X^{T} \beta_0 + u.
          \end{align*}
    \item If $g(\cdot)$ is the CDF of Normal$(0, 1)$,
          then the model turns out to be a probit model.
          \begin{itemize}
            \item See the textbook for further discussions on a probit model.
          \end{itemize}
    \item If $g(\cdot)$ is the CDF of logistic distribution,
          then the model turns out to be a logistic regression model.
  \end{itemize}
  
\end{frame}


%%%%%目次のページ%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Agenda}                  
  \tableofcontents
\end{frame}
%本文中に挿入するSECTION環境によって定義された目次が自動で反映される 


\section{Identification Conditions}

\begin{frame}{Identification Conditions}
  \begin{itembox}[l]{Proposition 8.1 (Identification of a Single Index Model)}
    \quad 
    For the semiparametric single index model $Y = g(x^{T} \beta_0) + u$,
    identification of $\beta_0$ and $g(\cdot)$ requires that
    \begin{itemize}
      \item (i)
            $x$ should not contain a constant/intercept, 
            and must contain at least one continuous variable.
            Moreover, $\|\beta_0\|$=1.
      \item (ii)
            $g(\cdot)$ is differentiable 
            and is not a constant function on the support of $x^{T}\beta_0$.
      \item (iii)
            For the discrete components of $x$, 
            varying the values of the discrete variables will not divide the support of $x^{T}\beta_0$ into disjoint subsets.
    \end{itemize}
  \end{itembox}
\end{frame}

\begin{frame}{Identification Condition (i)}
\begin{itemize}
  \item Note that the location and the scale of $\beta_0$ are not identified.
  \item The vector $x$ cannot include an intercept 
        because the function $g(\cdot)$ (which is to be estimated in nonparametric manners) includes any location and level shift.
        \begin{itemize}
          \item That is, $\beta_0$ cannot contain a location parameter.
        \end{itemize}
  \item Some normalization criterion (scale restrictions) for $\beta_0$ are needed.
        \begin{itemize}
          \item One approach is to set $\| \beta_0 \|$.
          \item The second approach is to set one component of $\beta_0$ to equal one. 
                This approach requires that 
                the variable corresponding to the component set to equal one 
                is continuously distributed 
                and has a non-zero coefficient.
          \item Then, $x$ must be dimension $2$ or larger. 
                If $x$ is one-dimensional, then $\beta_0 \in \mathbb{R}^1$ is simply normalized to 1, 
                and the model is the one-dimensional nonparametric regression $E(Y \mid x) = g(x)$ with no semiparametric component.
        \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Identification Conditions (ii) and (iii)}
\begin{itemize}
  \item The function $g(\cdot)$ cannot be a constant function and must be differentiable on the support of $x^{T}\beta_0$.
  \item $x$ must contain at least one continuously distributed variable
        and this continuous variable must have non-zero coefficient.
        \begin{itemize}
          \item  If not, $x^{T} \beta_0$ only takes a discrete set of values and it would be impossible to identify a continuous function $g(\cdot)$ on this discrete support.
        \end{itemize}
\end{itemize}
\end{frame}


\section{Estimation: Ichimura (1993)'s Method}


\begin{frame}{Ichimura (1993)'s Method: Semiparametric Least Squares}
\begin{itemize}
  \item Suppose that the functional form of $g(\cdot)$ were known.
  \item Then we could estimate $\beta_0$ by minimizing the least-squares criterion:
        \begin{align*}
        \sum_{i=1} \left[ Y_i - g(X_i^{T}\beta) \right]^2
        \end{align*}
        with respect to $\beta$.
  \item We could think about replacing $g(\cdot)$ with a nonparametric estimator $\hat{g}(\cdot)$.
  \item However, since $g(z)$ is the conditional mean of $Y_i$ given $X_i^{T} \beta_0 = z$,
        $g(\cdot)$ depends on unknown $\beta_0$, so we cannot estimate $g(\cdot)$ here.
\end{itemize}
\end{frame}

\begin{frame}{Ichimura (1993)'s Method: Semiparametric Least Squares}
\begin{itemize}
  \item Nevertheless, for a fixed value of $\beta$, we can estimate
        \begin{align*}
        G(X_i^{T} \beta) 
          := \mathbb{E} (Y_i \mid X_i^{T}\beta) 
           = \mathbb{E} (g(X_i^{T}\beta) \mid X_i^{T}\beta).
        \end{align*}
  \item In general $G(X_i^{T}\beta) \neq g(X_i^{T} \beta)$.
  \item When $\beta = \beta_0$ \footnote{Recall that $\beta_0$ is the true value of $\beta$.},
        it holds that $G(X_i^{T}\beta_0) = g(X_i^{T} \beta_0)$. 
\end{itemize}  
\end{frame}

\begin{frame}{Ichimura (1993)'s Method: Semiparametric Least Squares}
  \begin{itemize}
  \item First, we estimate $G(X_i^{T}\beta)$  
        with the leave-one-out NW estimator:
        \begin{align*}
          \hat{G}_{-i}(X_i^{T}\beta) 
              :&= \hat{\mathbb{E}}_{-i}(Y_i \mid X_i^{T} \beta) \\
               &= \frac
                  {\sum_{j \neq i} Y_j 
                   K \left( \frac
                            {X_j^{T}\beta - X_i^{T}\beta}
                            {h} 
                     \right) 
                  }
                  {\sum_{j \neq i} 
                   K \left( \frac
                           {X_j^{T}\beta - X_i^{T}\beta}
                           {h} 
                     \right) 
                  }.
        \end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Ichimura (1993)'s Method: Semiparametric Least Squares}
  \begin{itemize}
    \item Second, 
          using the leave-one-out NW estimator $\hat{G}_{-i}(X_i^{T}\beta)$,
          we estimate $\beta$ with
          \begin{align*}
            \hat{\beta} 
              := \arg \min_{\beta} 
                    \sum_{i=1}^{n}  
                          \left[ Y_i - \hat{G}_{-i}(X_i^{T}\beta) \right]^2 
                          w(X_i) \mathbf{1}(X_i \in A_n) := S_n(\beta).
          \end{align*}
    \item  $w(X_i)$ is a nonnegative weight function.
    \item  $\mathbf{1}(X_i \in A_n)$ is a trimming function to trim out small values of 
           $\frac{1}{nh} \sum_{j \neq i} 
              K \left( \frac
                       {X_j^{T}\beta - X_i^{T}\beta}
                       {h} 
                \right) $,
           so that we do not suffer the random dominator problem. 
  \end{itemize}
\end{frame}


\begin{frame}{Asymptotic Distribution of Ichimura (1993)'s Estimator}
\begin{itemize}
  \item See Theorem 8.1. for the statement on the asymptotic distribution of Ichimura's estimator.
\end{itemize}
\end{frame}


\section{Direct Semiparametric Estimators for $\beta$}










\section{Bandwidth Selection}









\section{Klein and Spady (1993)}









\section{Lewbel (2000)}










\section{Manski's (1975) Maximum Score Estimator}









\section{Horowitz's (1992) Smoothed Maximum Score Estimator}











\section{Han's (1987) Maximum Rank Estimator}










\section{Multinomial Discrete Choice Models}









\section{Ai's (1997) Semiparametric Maximum Likelihood Approach}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{References}


\begin{frame}{References (1)}
  \begin{itemize}
    \item Li, Q. and J. S. Racine, (2007). 
          \textit{Nonparametric Econometrics: Theory and Practice,} 
          Princeton University Press.
    \item 末石直也 (2024) 『データ駆動型回帰分析：計量経済学と機械学習の融合』日本評論社．
    \item 西山慶彦，人見光太郎 (2023) 『ノン・セミパラメトリック統計解析（理論統計学教程：数理統計の枠組み）』共立出版．
  \end{itemize}
\end{frame}

\begin{frame}{References (2)}
\quad 
Useful references also include some lecture notes of the following topic courses:
  \begin{itemize}
    \item ECON 718 NonParametric Econometrics (Bruce Hansen, Spring 2009, University of Wisconsin-Madison),
    \item セミノンパラメトリック計量分析（末石直也，2014年度後期，京都大学）．
  \end{itemize}
\end{frame}

\end{document}